{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b564500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, skew\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def perform_EDA(df, filename):\n",
    "    print(f\"=== {filename} EDA Report ===\")\n",
    "    \n",
    "    # Record counts\n",
    "    print(f\"\\nNumber of records: {len(df)}\")\n",
    "    \n",
    "    # Duplicate Records\n",
    "    duplicates = len(df) - len(df.drop_duplicates())\n",
    "    print(f\"Number of duplicate records: {duplicates}\")\n",
    "    \n",
    "    # Dataframe Info\n",
    "    print(f\"\\nDataFrame Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Descriptive Stats for Numeric Columns\n",
    "    print(f\"\\nDescriptive statistics for numeric columns:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Integers\n",
    "    integer_columns = df.select_dtypes(include='int64').columns.tolist()\n",
    "    if integer_columns:\n",
    "        print(f\"\\nInteger data type columns: {', '.join(integer_columns)}\")\n",
    "    \n",
    "    # Floats\n",
    "    float_columns = df.select_dtypes(include='float64').columns.tolist()\n",
    "    if float_columns:\n",
    "        print(f\"Float data type columns: {', '.join(float_columns)}\")\n",
    "   \n",
    "    # Objects\n",
    "    print(\"\\nObject Data Info:\")\n",
    "    object_data = [{\n",
    "        \"Column Name\": col, \n",
    "        \"Number of Unique Values\": df[col].nunique(), \n",
    "        \"Average String Length\": round(df[col].str.len().mean(), 2),\n",
    "        \"Max String Length\": df[col].str.len().max(),\n",
    "        \"Min String Length\": df[col].str.len().min()\n",
    "    } for col in df.select_dtypes(include='object').columns]\n",
    "    print(pd.DataFrame(object_data))\n",
    "    \n",
    "    # Boolean\n",
    "    print(\"\\nBoolean Data Info:\")\n",
    "    boolean_data = [{\n",
    "        \"Column Name\": col,\n",
    "        \"Number of True Values\": df[col].sum(),\n",
    "        \"Number of False Values\": len(df) - df[col].sum()\n",
    "    } for col in df.select_dtypes(include='bool').columns]\n",
    "    print(pd.DataFrame(boolean_data))\n",
    "    \n",
    "    # Rows with Null Values\n",
    "    null_columns = df.columns[df.isnull().any()].tolist()\n",
    "    print(f\"\\nColumns with null values: {', '.join(null_columns)}\")\n",
    "    \n",
    "    null_rows = df.isnull().any(axis=1).sum()\n",
    "    print(f\"Rows with null values: {null_rows}\")\n",
    "\n",
    "    # Correlation Coefficients between Numeric Columns\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    correlations = {}\n",
    "    for col in numeric_columns:\n",
    "        if col != 'totalFare' and df[col].dtype in ['float64', 'int64']:\n",
    "            correlation, _ = pearsonr(df[col], df['totalFare'])\n",
    "            correlations[col] = correlation\n",
    "    print(\"\\nCorrelation Coefficients:\")\n",
    "    print(correlations)\n",
    "\n",
    "    # High Cardinality\n",
    "    unique_counts = df.nunique()\n",
    "    total_records = len(df)\n",
    "    high_cardinality_cols = unique_counts[unique_counts / total_records > 0.9].index.tolist()\n",
    "    print(\"\\nHigh Cardinality Columns:\")\n",
    "    print(high_cardinality_cols)\n",
    "    \n",
    "    # Skewness\n",
    "    skewed_cols = {col: skew(df[col].dropna()) for col in numeric_columns}\n",
    "    print(\"\\nSkewness in Numeric Columns:\")\n",
    "    print(skewed_cols)\n",
    "\n",
    "    # Visualizations\n",
    "    dp = sns.displot(df['totalFare'], kde=True, bins=30)\n",
    "    dp.set_axis_labels(\"Total Fare\", \"Frequency\")\n",
    "    plt.title(\"Distribution of Total Fare\")\n",
    "    plt.show()\n",
    "\n",
    "    lp = sns.lmplot(x='totalTravelDistance', y='totalFare', data=df)\n",
    "    lp.set_axis_labels(\"Total Travel Distance\", \"Total Fare\")\n",
    "    plt.title(\"Relationship Between Total Travel Distance and Total Fare\")\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    sns.heatmap(df[numeric_columns].corr(), annot=True, cmap=\"YlGnBu\")\n",
    "    plt.title('Correlation Heatmap of Numeric Features')\n",
    "    plt.show()\n",
    "\n",
    "# File paths & names\n",
    "gcs_bucket = \"gs://project-bucket-kl/landing\"\n",
    "filename_list = ['itineraries.csv']\n",
    "\n",
    "for filename in filename_list:\n",
    "    print(f\"\\nWorking on file: {filename}\")\n",
    "    gcs_path = f\"{gcs_bucket}/{filename}\"\n",
    "    df = pd.read_csv(gcs_path, sep=',', on_bad_lines='skip', nrows=10000)\n",
    "    perform_EDA(df, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_date\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "gcs_bucket = \"gs://project-bucket-kl/landing\"\n",
    "filename = \"itineraries.csv\"\n",
    "gcs_path = f\"{gcs_bucket}/{filename}\"\n",
    "df = spark.read.csv(gcs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display initial record count\n",
    "initial_count = df.count()\n",
    "print(f\"Total number of records before cleaning: {initial_count}\")\n",
    "\n",
    "# Drop Rows with Null Values in Critical Columns\n",
    "critical_columns = ['searchDate', 'flightDate', 'startingAirport', 'destinationAirport', \n",
    "                    'baseFare', 'totalFare', 'travelDuration', 'totalTravelDistance']\n",
    "df = df.dropna(subset=critical_columns)\n",
    "\n",
    "# Drop Unnecessary Columns\n",
    "drop_columns = [\"legId\", \"fareBasisCode\", \"segmentsDepartureTimeRaw\", \n",
    "                \"segmentsDepartureTimeEpochSeconds\", \"segmentsArrivalTimeRaw\", \n",
    "                \"segmentsArrivalTimeEpochSeconds\", \"segmentsEquipmentDescription\", \n",
    "                \"segmentsDurationInSeconds\"]\n",
    "\n",
    "df = df.drop(*drop_columns)\n",
    "\n",
    "# Convert date columns to proper date format\n",
    "date_columns = [\"searchDate\", \"flightDate\"]\n",
    "for date_col in date_columns:\n",
    "    df = df.withColumn(date_col, to_date(col(date_col), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Function to convert duration string to minutes\n",
    "def duration_to_minutes(duration_str):\n",
    "    try:\n",
    "        total_minutes = 0\n",
    "        if 'H' in duration_str:\n",
    "            total_minutes += int(re.search(r'(\\d+)H', duration_str).group(1)) * 60\n",
    "        if 'M' in duration_str:\n",
    "            total_minutes += int(re.search(r'(\\d+)M', duration_str).group(1))\n",
    "        return total_minutes\n",
    "    except:\n",
    "        return None  # Returning None will cause the row to be dropped later\n",
    "\n",
    "to_minutes_udf = udf(duration_to_minutes, IntegerType())\n",
    "\n",
    "# Convert 'travelDuration' to minutes\n",
    "df = df.withColumn(\"travelDurationMinutes\", to_minutes_udf(\"travelDuration\"))\n",
    "df = df.dropna(subset=[\"travelDurationMinutes\"])\n",
    "\n",
    "# Convert fare and distance columns to double\n",
    "fare_and_distance_columns = [\"baseFare\", \"totalFare\", \"totalTravelDistance\"]\n",
    "for col_name in fare_and_distance_columns:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "# Final data count after cleaning\n",
    "final_count = df.count()\n",
    "print(f\"Total number of records after cleaning: {final_count}\")\n",
    "\n",
    "# Save Cleaned Data\n",
    "output_path = \"gs://project-bucket-kl/cleaned/cleaned_data.parquet\"\n",
    "df.write.mode('overwrite').parquet(output_path)\n",
    "print(\"Cleaned data saved to Parquet file.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
