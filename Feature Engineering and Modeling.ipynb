{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56074fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType, DateType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import dayofweek, hour\n",
    "\n",
    "# Schema\n",
    "schema = StructType([\n",
    "    StructField(\"flightDate\", DateType()),\n",
    "    StructField(\"startingAirport\", StringType()),\n",
    "    StructField(\"destinationAirport\", StringType()),\n",
    "    StructField(\"isBasicEconomy\", BooleanType()),\n",
    "    StructField(\"isNonStop\", BooleanType()),\n",
    "    StructField(\"segmentsAirlineName\", StringType()),\n",
    "    StructField(\"baseFare\", DoubleType()),\n",
    "    StructField(\"totalFare\", DoubleType()),\n",
    "    StructField(\"totalTravelDistance\", DoubleType()),\n",
    "    StructField(\"travelDurationMinutes\", IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).parquet(\"gs://project-bucket-kl/cleaned/cleaned_data.parquet\")\n",
    "\n",
    "# Feature Engineering\n",
    "df = df.withColumn(\"dayOfWeek\", dayofweek(df.flightDate))\n",
    "df = df.withColumn(\"hourOfDay\", hour(df.flightDate))\n",
    "df = df.withColumn(\"isBasicEconomy\", df.isBasicEconomy.cast(StringType()))\n",
    "df = df.withColumn(\"isNonStop\", df.isNonStop.cast(StringType()))\n",
    "\n",
    "# Indexers and Encoders for categorical features\n",
    "categoricalColumns = [\"startingAirport\", \"destinationAirport\", \"isBasicEconomy\", \"isNonStop\", \"segmentsAirlineName\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_indexed\") for column in categoricalColumns]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[indexer.getOutputCol().replace(\"_indexed\", \"_encoded\") for indexer in indexers]\n",
    ")\n",
    "\n",
    "# Numeric Columns to Vectors\n",
    "numeric_features = [\"baseFare\", \"totalTravelDistance\", \"travelDurationMinutes\"]\n",
    "vector_assemblers = [VectorAssembler(inputCols=[feature], outputCol=feature + \"Vec\") for feature in numeric_features]\n",
    "\n",
    "# Normalize numeric columns using MinMaxScaler\n",
    "scalers = [MinMaxScaler(inputCol=feature + \"Vec\", outputCol=feature + \"_scaled\") for feature in numeric_features]\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[enc for enc in encoder.getOutputCols()] + [scaler.getOutputCol() for scaler in scalers],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [encoder] + vector_assemblers + scalers + [assembler])\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "#Save Transformed Data\n",
    "df_transformed.write.mode('overwrite').parquet(\"gs://project-bucket-kl/trusted/feature_engineered_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType, DateType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import dayofweek, hour\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"flightDate\", DateType()),\n",
    "    StructField(\"startingAirport\", StringType()),\n",
    "    StructField(\"destinationAirport\", StringType()),\n",
    "    StructField(\"isBasicEconomy\", BooleanType()),\n",
    "    StructField(\"isNonStop\", BooleanType()),\n",
    "    StructField(\"segmentsAirlineName\", StringType()),\n",
    "    StructField(\"baseFare\", DoubleType()),\n",
    "    StructField(\"totalFare\", DoubleType()),\n",
    "    StructField(\"totalTravelDistance\", DoubleType()),\n",
    "    StructField(\"travelDurationMinutes\", IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).parquet(\"gs://project-bucket-kl/trusted/feature_engineered_data.parquet\")\n",
    "\n",
    "print(\"Defining and training the model...\")\n",
    "# Model definition\n",
    "lr = LinearRegression(featuresCol='features', labelCol='totalFare', regParam=0.1, elasticNetParam=0.5, maxIter=50)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "print(\"Making predictions and evaluating the model...\")\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"totalFare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "print(\"Saving the model...\")\n",
    "# Save the model\n",
    "model.write().overwrite().save(\"gs://project-bucket-kl/models/flight_price_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "data_path = \"gs://project-bucket-kl/trusted/feature_engineered_data.parquet\"\n",
    "df = spark.read.parquet(data_path)\n",
    "\n",
    "# Define Linear Regression model and parameter grid\n",
    "lr = LinearRegression(featuresCol='features', labelCol='totalFare')\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Define Evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"totalFare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    seed=42)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Get Best Model\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Predictions on test data\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate final model\n",
    "final_rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {final_rmse}\")\n",
    "\n",
    "# Hyperparameters of best model\n",
    "print(f\"Best Model Hyperparameters:\")\n",
    "print(f\" - regParam: {bestModel._java_obj.getRegParam()}\")\n",
    "print(f\" - elasticNetParam: {bestModel._java_obj.getElasticNetParam()}\")\n",
    "print(f\" - maxIter: {bestModel._java_obj.getMaxIter()}\")\n",
    "\n",
    "# Save the best model\n",
    "final_model_path = \"gs://project-bucket-kl/models/flight_price_best_model\"\n",
    "bestModel.write().overwrite().save(final_model_path)\n",
    "print(f\"Model saved successfully at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import dayofweek, round\n",
    "\n",
    "data = spark.read.parquet(\"gs://project-bucket-kl/trusted/feature_engineered_data.parquet\")\n",
    "model_path = \"gs://project-bucket-kl/models/flight_price_model\"\n",
    "model = LinearRegressionModel.load(model_path)\n",
    "\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Round the prediction results to the nearest hundredth\n",
    "predictions = predictions.withColumn(\"roundedPrediction\", round(predictions.prediction, 2))\n",
    "\n",
    "# Display\n",
    "predictions.select(\n",
    "    \"totalFare\",\n",
    "    \"roundedPrediction\",\n",
    "    \"startingAirport\", \n",
    "    \"destinationAirport\",\n",
    "    \"totalTravelDistance\",\n",
    "    \"travelDurationMinutes\",\n",
    "    \"flightDate\",\n",
    "    \"segmentsAirlineName\",\n",
    "    \"dayOfWeek\"\n",
    ").show(10)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"totalFare\", predictionCol=\"roundedPrediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on the data = {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
